{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Project Submission\n",
    "\n",
    "Please fill out:\n",
    "* Student name: Gabe Vazquez\n",
    "* Student pace: part time \n",
    "* Scheduled project review date/time: 1/24: 6PM\n",
    "* Instructor name: Jeff Herman\n",
    "* Blog post URL:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>10/13/2014</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>12/9/2014</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400.0</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>2/25/2015</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1933</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>12/9/2014</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910.0</td>\n",
       "      <td>1965</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>2/18/2015</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id        date     price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  10/13/2014  221900.0         3       1.00         1180   \n",
       "1  6414100192   12/9/2014  538000.0         3       2.25         2570   \n",
       "2  5631500400   2/25/2015  180000.0         2       1.00          770   \n",
       "3  2487200875   12/9/2014  604000.0         4       3.00         1960   \n",
       "4  1954400510   2/18/2015  510000.0         3       2.00         1680   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view     ...      grade  sqft_above  \\\n",
       "0      5650     1.0         NaN   0.0     ...          7        1180   \n",
       "1      7242     2.0         0.0   0.0     ...          7        2170   \n",
       "2     10000     1.0         0.0   0.0     ...          6         770   \n",
       "3      5000     1.0         0.0   0.0     ...          7        1050   \n",
       "4      8080     1.0         0.0   0.0     ...          8        1680   \n",
       "\n",
       "   sqft_basement yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0            0.0     1955           0.0    98178  47.5112 -122.257   \n",
       "1          400.0     1951        1991.0    98125  47.7210 -122.319   \n",
       "2            0.0     1933           NaN    98028  47.7379 -122.233   \n",
       "3          910.0     1965           0.0    98136  47.5208 -122.393   \n",
       "4            0.0     1987           0.0    98074  47.6168 -122.045   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the data\n",
    "data = pd.read_csv('kc_house_data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import neccessary packages asd set them to their standard alias\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checklist:\n",
    "\n",
    "1. Check data types and null values.\n",
    "* Check if we need/ can create a subset of the data.\n",
    "* Check for multicollinearity by determining if we have columns that are highly correlated\n",
    "* Should any columns be removed becuase they do not contain useful information?\n",
    "* Scale / Normalize our numeric data?\n",
    "* Deal with categorical columns by one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Check data types and null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 21597 entries, 0 to 21596\n",
      "Data columns (total 21 columns):\n",
      "id               21597 non-null int64\n",
      "date             21597 non-null object\n",
      "price            21597 non-null float64\n",
      "bedrooms         21597 non-null int64\n",
      "bathrooms        21597 non-null float64\n",
      "sqft_living      21597 non-null int64\n",
      "sqft_lot         21597 non-null int64\n",
      "floors           21597 non-null float64\n",
      "waterfront       19221 non-null float64\n",
      "view             21534 non-null float64\n",
      "condition        21597 non-null int64\n",
      "grade            21597 non-null int64\n",
      "sqft_above       21597 non-null int64\n",
      "sqft_basement    21597 non-null object\n",
      "yr_built         21597 non-null int64\n",
      "yr_renovated     17755 non-null float64\n",
      "zipcode          21597 non-null int64\n",
      "lat              21597 non-null float64\n",
      "long             21597 non-null float64\n",
      "sqft_living15    21597 non-null int64\n",
      "sqft_lot15       21597 non-null int64\n",
      "dtypes: float64(8), int64(11), object(2)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invetigate the unique values inside of `condition` and `grade` becuase these might need to be set to categorical data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition:\n",
      "[3 5 4 1 2]\n",
      "Grade:\n",
      "[ 7  6  8 11  9  5 10 12  4  3 13]\n"
     ]
    }
   ],
   "source": [
    "print(\"Condition:\")\n",
    "print(data.condition.unique())\n",
    "\n",
    "print(\"Grade:\")\n",
    "print(data.grade.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that `condition` and `grade` should be categories instead of intergers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condition:  object\n",
      "Grade:  object\n"
     ]
    }
   ],
   "source": [
    "data.condition = data.condition.astype(\"str\")\n",
    "print(\"Condition: \", data.condition.dtype)\n",
    "\n",
    "data.grade = data.grade.astype(\"str\")\n",
    "print(\"Grade: \", data.grade.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that `sqft_basement` should be converted to a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0.0', '400.0', '910.0', '1530.0', '?', '730.0', '1700.0', '300.0',\n",
       "       '970.0', '760.0', '720.0', '700.0', '820.0', '780.0', '790.0',\n",
       "       '330.0', '1620.0', '360.0', '588.0', '1510.0', '410.0', '990.0',\n",
       "       '600.0', '560.0', '550.0', '1000.0', '1600.0', '500.0', '1040.0',\n",
       "       '880.0', '1010.0', '240.0', '265.0', '290.0', '800.0', '540.0',\n",
       "       '710.0', '840.0', '380.0', '770.0', '480.0', '570.0', '1490.0',\n",
       "       '620.0', '1250.0', '1270.0', '120.0', '650.0', '180.0', '1130.0',\n",
       "       '450.0', '1640.0', '1460.0', '1020.0', '1030.0', '750.0', '640.0',\n",
       "       '1070.0', '490.0', '1310.0', '630.0', '2000.0', '390.0', '430.0',\n",
       "       '850.0', '210.0', '1430.0', '1950.0', '440.0', '220.0', '1160.0',\n",
       "       '860.0', '580.0', '2060.0', '1820.0', '1180.0', '200.0', '1150.0',\n",
       "       '1200.0', '680.0', '530.0', '1450.0', '1170.0', '1080.0', '960.0',\n",
       "       '280.0', '870.0', '1100.0', '460.0', '1400.0', '660.0', '1220.0',\n",
       "       '900.0', '420.0', '1580.0', '1380.0', '475.0', '690.0', '270.0',\n",
       "       '350.0', '935.0', '1370.0', '980.0', '1470.0', '160.0', '950.0',\n",
       "       '50.0', '740.0', '1780.0', '1900.0', '340.0', '470.0', '370.0',\n",
       "       '140.0', '1760.0', '130.0', '520.0', '890.0', '1110.0', '150.0',\n",
       "       '1720.0', '810.0', '190.0', '1290.0', '670.0', '1800.0', '1120.0',\n",
       "       '1810.0', '60.0', '1050.0', '940.0', '310.0', '930.0', '1390.0',\n",
       "       '610.0', '1830.0', '1300.0', '510.0', '1330.0', '1590.0', '920.0',\n",
       "       '1320.0', '1420.0', '1240.0', '1960.0', '1560.0', '2020.0',\n",
       "       '1190.0', '2110.0', '1280.0', '250.0', '2390.0', '1230.0', '170.0',\n",
       "       '830.0', '1260.0', '1410.0', '1340.0', '590.0', '1500.0', '1140.0',\n",
       "       '260.0', '100.0', '320.0', '1480.0', '1060.0', '1284.0', '1670.0',\n",
       "       '1350.0', '2570.0', '1090.0', '110.0', '2500.0', '90.0', '1940.0',\n",
       "       '1550.0', '2350.0', '2490.0', '1481.0', '1360.0', '1135.0',\n",
       "       '1520.0', '1850.0', '1660.0', '2130.0', '2600.0', '1690.0',\n",
       "       '243.0', '1210.0', '1024.0', '1798.0', '1610.0', '1440.0',\n",
       "       '1570.0', '1650.0', '704.0', '1910.0', '1630.0', '2360.0',\n",
       "       '1852.0', '2090.0', '2400.0', '1790.0', '2150.0', '230.0', '70.0',\n",
       "       '1680.0', '2100.0', '3000.0', '1870.0', '1710.0', '2030.0',\n",
       "       '875.0', '1540.0', '2850.0', '2170.0', '506.0', '906.0', '145.0',\n",
       "       '2040.0', '784.0', '1750.0', '374.0', '518.0', '2720.0', '2730.0',\n",
       "       '1840.0', '3480.0', '2160.0', '1920.0', '2330.0', '1860.0',\n",
       "       '2050.0', '4820.0', '1913.0', '80.0', '2010.0', '3260.0', '2200.0',\n",
       "       '415.0', '1730.0', '652.0', '2196.0', '1930.0', '515.0', '40.0',\n",
       "       '2080.0', '2580.0', '1548.0', '1740.0', '235.0', '861.0', '1890.0',\n",
       "       '2220.0', '792.0', '2070.0', '4130.0', '2250.0', '2240.0',\n",
       "       '1990.0', '768.0', '2550.0', '435.0', '1008.0', '2300.0', '2610.0',\n",
       "       '666.0', '3500.0', '172.0', '1816.0', '2190.0', '1245.0', '1525.0',\n",
       "       '1880.0', '862.0', '946.0', '1281.0', '414.0', '2180.0', '276.0',\n",
       "       '1248.0', '602.0', '516.0', '176.0', '225.0', '1275.0', '266.0',\n",
       "       '283.0', '65.0', '2310.0', '10.0', '1770.0', '2120.0', '295.0',\n",
       "       '207.0', '915.0', '556.0', '417.0', '143.0', '508.0', '2810.0',\n",
       "       '20.0', '274.0', '248.0'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sqft_basement.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`?` won't be able to be converted to a numeric value. Therefore, change all `?` values to `0`s in order to convert this column to floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data[data.sqft_basement == '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqft_basement_no_ques = data[data.sqft_basement != '?']\n",
    "len(sqft_basement_no_ques[sqft_basement_no_ques.sqft_basement == '?'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqft_basement_no_ques.sqft_basement = sqft_basement_no_ques.sqft_basement.astype(float)\n",
    "print(sqft_basement_no_ques.sqft_basement.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqft_basement_no_ques.sqft_basement.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqft_basement_no_ques.sqft_basement.median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['sqft_basement'] = data['sqft_basement'].replace('?', 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sqft_basement = data.sqft_basement.astype(\"float\")\n",
    "print(\"Sqft Basement: \", data.sqft_basement.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further inspect the unique values in `view`, `waterfront` and `yr_renovated` to determine its value to the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"-----View-----\")\n",
    "print(data.view.unique())\n",
    "print(data.view.value_counts())\n",
    "\n",
    "print(\"-----Waterfront-----\")\n",
    "print(data.waterfront.unique())\n",
    "print(data.waterfront.value_counts())\n",
    "\n",
    "print(\"-----Year Renovated-----\")\n",
    "print(data.yr_renovated.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because there are so few null values for `view` in prportion the rest of the data set, these null rows can simply be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna(subset=['view'])\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining `view` values can be converted to categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.view = data.view.astype(\"str\")\n",
    "print(\"View: \", data.view.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `waterfront` to categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.waterfront = data.waterfront.astype(\"str\")\n",
    "print(\"Waterfront: \", data.waterfront.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `yr_renovated` null values to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['yr_renovated'] = data['yr_renovated'].fillna(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm no more `null` values exist in the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the data set to ensure all else has been maintained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Check if we need/ can create a subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large enough data set, to build a model on a subset of the data in order to increase the speed during the modeling step. \n",
    "\n",
    "The `NaN` string values that have been created in the step prior can be removed in order to create this subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data.waterfront != 'nan']\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Check for multicollinearity by determining if we have columns that are highly correlated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "#Create Covariance matrix\n",
    "corr = data.corr()\n",
    "\n",
    "# Generate a mask the size of the covariance matrix (to prevent invalid data)\n",
    "mask = np.zeros_like(corr, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "#Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Gernerate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0, \n",
    "            square=True, linewidth=.5, cbar_kws={\"shrink\":.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Should any columns be removed becuase they do not contain useful information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`id` does not have any correlation with most of the data set, so it is best to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop('id', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Normalize our numeric data (using their z-score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=[16,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed = ['bedrooms', 'price', 'sqft_above', 'sqft_basement', 'sqft_living', 'sqft_living', 'sqft_lot', 'yr_renovated']\n",
    "\n",
    "for predictor in skewed:\n",
    "    data[predictor] = np.log(data[predictor])\n",
    "    \n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how many negative values there are in the `sqft_basement` and `yr_renovated` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(data[\"sqft_basement\"]<=0))\n",
    "print(sum(data[\"yr_renovated\"]<=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if makes sense to convert them to their respective medians so as to not disrupt the rest of the aligned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basement_median = data.sqft_basement.median()\n",
    "yr_renovated_median = data.yr_renovated.median()\n",
    "\n",
    "print(basement_median)\n",
    "print(yr_renovated_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert these numbers the neg numbers to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.sqft_basement:\n",
    "    if i <= 0:\n",
    "        data.sqft_basement = data['sqft_basement'].replace(i, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data.yr_renovated:\n",
    "    if i <= 0:\n",
    "        data.yr_renovated = data['yr_renovated'].replace(i, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further normalize the data with the z scores of the log- transformed dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.price = (data.price - data.price.mean()) / data.price.std()\n",
    "data.bedrooms = (data.bedrooms - data.bedrooms.mean()) / data.bedrooms.std()\n",
    "data.bathrooms = (data.bathrooms - data.bathrooms.mean()) / data.bathrooms.std()\n",
    "data.sqft_living = (data.sqft_living - data.sqft_living.mean()) / data.sqft_living.std()\n",
    "data.sqft_lot = (data.sqft_lot - data.sqft_lot.mean()) / data.sqft_lot.std()\n",
    "data.floors = (data.floors - data.floors.mean()) / data.floors.std()\n",
    "data.sqft_above = (data.sqft_above - data.sqft_above.mean()) / data.sqft_above.std()\n",
    "data.sqft_basement = (data.sqft_basement - data.sqft_basement.mean()) / data.sqft_basement.std()\n",
    "data.yr_built = (data.yr_built - data.yr_built.mean()) / data.yr_built.std()\n",
    "data.yr_renovated = (data.yr_renovated - data.yr_renovated.mean()) / data.yr_renovated.std()\n",
    "data.lat = (data.lat - data.lat.mean()) / data.lat.std()\n",
    "data.long = (data.long - data.long.mean()) / data.long.std()\n",
    "data.sqft_living15 = (data.sqft_living15 - data.sqft_living15.mean()) / data.sqft_living15.std()\n",
    "data.sqft_lot15 = (data.sqft_lot15 - data.sqft_lot15.mean()) / data.sqft_lot15.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=[16,12])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Deal with categorical columns by one-hot encoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be completed after data exploration so as to make it more convenient view data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = pd.get_dummies(data)\n",
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shows basic statistics for all the \"numerical\" columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(20,18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Most of the variables clearly break the normal assumption\n",
    "* There are some obvious outliers in some columns \n",
    "* Data might need extra preprocessing to clean it up a bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['bathrooms',\n",
    "       'bedrooms', 'floors', 'lat', 'long', 'price', \n",
    "               'sqft_above', 'sqft_basement', 'yr_built', \n",
    "               'yr_renovated', 'sqft_living', 'sqft_living15',\n",
    "              'sqft_lot', 'sqft_lot15']:\n",
    "    data[column].plot.hist(normed=True )\n",
    "    data[column].plot.kde(label=column )\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This further confirms our observations from  simple histograms\n",
    "* A number of distributions are multimodal - they have more than one \"typical values\"\n",
    "* We may need to generate separate plots while fine tuning the number of bins to view the data in a better way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further asses the data with a joint plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['bathrooms',\n",
    "       'bedrooms', 'floors', 'lat', 'long', 'price', \n",
    "               'sqft_above', 'sqft_basement', 'yr_built', \n",
    "               'yr_renovated', 'sqft_living', 'sqft_living15',\n",
    "              'sqft_lot', 'sqft_lot15']:\n",
    "    sns.jointplot(x=column, y=\"price\",\n",
    "                  data=data, \n",
    "                  kind='reg', \n",
    "                  label=column,\n",
    "                  joint_kws={'line_kws':{'color':'green'}})\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Joint plots further confirm our observations we made earlier.\n",
    "* Some distributions are *almost* uniformly distributed while other have multiple common values\n",
    "* `bathrooms`, `bedrooms`, `sqft_above`, `sqft_living`, and `sqft_living15` have strong positive correlation with price\n",
    "* `long`, `yr_built`, and `yr_renovated` almost have no clear relationship with sales.\n",
    "* For above variables we see a straightline along the y-intercept i.e. no slope > no linear relation\n",
    "* The `sqft` variables have the best poritive relationship with `price`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['date'] = pd.to_datetime(data['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month'] = data['date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['month']  = data['month'].astype(\"str\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('date', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = pd.get_dummies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View `info` for the numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.iloc[:,0:10].info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure all dummy columns are tagged as categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cat_names in data_cleaned.iloc[:,10:].columns:\n",
    "         data_cleaned[cat_names] = data_cleaned[cat_names].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.view.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.describe().columns.drop(['price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names = data_cleaned.describe().columns.drop(['price'])\n",
    "results = [['ind_var', 'r_squared', 'intercept', 'slope', 'p-value' ]]\n",
    "for idx, val in enumerate(col_names):\n",
    "    print (\"KC House: price~\" + val)\n",
    "    print (\"------------------------------\")\n",
    "\n",
    "    f = 'price~' + val\n",
    "    model = smf.ols(formula=f, data=data_cleaned).fit()\n",
    "    X_new = pd.DataFrame({val: [data_cleaned[val].min(), data_cleaned[val].max()]});\n",
    "    preds = model.predict(X_new)\n",
    "    results.append([val, model.rsquared, model.params[0], model.params[1], model.pvalues[1] ])\n",
    "    print(results[idx+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What do the parameter estimates mean? Do they make sense?\n",
    "* What do the p-values tell us?\n",
    "* What does the R-squared tell us?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`yr_renovated` is the most skewed. Mean-transform to see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mt_yr = ((data_cleaned.yr_renovated - data_cleaned.yr_renovated.mean()) / data_cleaned.yr_renovated.std())\n",
    "mt_yr.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert `yr_renovated` to categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned.yr_renovated = data_cleaned.yr_renovated.astype(\"str\")\n",
    "print(\"Yr_Renovated: \", data_cleaned.yr_renovated.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data_cleaned.columns)):\n",
    "               print(i,data_cleaned.columns[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename all columns that have a '.' or '/'. They will hinder our model from running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data_cleaned.rename(columns={'waterfront_0.0': 'waterfront_0', 'waterfront_1.0': 'waterfront_1', \n",
    "                                            'view_0.0': 'view_0', 'view_1.0': 'view_1', 'view_2.0': 'view_2',\n",
    "                                           'view_3.0': 'view_3', 'view_4.0': 'view_4'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(data_cleaned.columns)):\n",
    "               data_cleaned.columns[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_waterfront = data_cleaned.columns[15:16]\n",
    "cols_view = data_cleaned.columns[17:21]\n",
    "cols_condition = data_cleaned.columns[22:26]\n",
    "cols_grade = data_cleaned.columns[27:37]\n",
    "cols_month= data_cleaned.columns[38:49]\n",
    "\n",
    "cols = [cols_waterfront , cols_view, cols_condition, cols_grade, cols_month]\n",
    "for col in cols:\n",
    "    sum_cols = \"+\".join(col)\n",
    "    form = \"price ~\" + sum_cols\n",
    "    model = smf.ols(formula= form, data= data_cleaned).fit()\n",
    "    print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Drop continuous variables which resulted in single linear models with a R-squared value < 0.01.\n",
    "* Drop 1 column for each categorical variable we end up using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kc_house_final = data_cleaned.drop([\"yr_built\"], axis=1)\n",
    "kc_house_final.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature ranking with recursive feature elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix X and y containing the predictors and target for the model. Use Scikit-Learn's RFE function, documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = kc_house_final[[\"price\"]]\n",
    "X = kc_house_final.drop([\"price\"], axis=1)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a for loop using RFE where we look at the 5, 15, 25,... up until 85 best features to be selected according to the feature ranking algorithm. Store the R-squared and the adjusted-R-squareds for all these models in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_list = []\n",
    "adj_r_list = []\n",
    "list_n = list(range(5,45,5))\n",
    "for n in list_n: \n",
    "    select_n = RFE(linreg, n_features_to_select = n)\n",
    "    select_n = select_n.fit(X, np.ravel(y))\n",
    "    selected_columns = X.columns[select_n.support_ ]\n",
    "    linreg.fit(X[selected_columns],y)\n",
    "    yhat = linreg.predict(X[selected_columns])\n",
    "    SS_Residual = np.sum((y-yhat)**2)\n",
    "    SS_Total = np.sum((y-np.mean(y))**2)\n",
    "    r_squared = 1 - (float(SS_Residual))/SS_Total\n",
    "    print(r_squared)\n",
    "    adjusted_r_squared = 1 - (1-r_squared)*(len(y)-1)/(len(y)-X.shape[1]-1)\n",
    "    print(adjusted_r_squared)\n",
    "r_list.append(r_squared)\n",
    "adj_r_list.append(adjusted_r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between $R^2$ and adjusted $R^2$ is negligible, and seems to continue to be going up as we include more features. Remember though that we're likely overfitting when including 85 features. In order to identify this, let's rerun a similar experiment, but using a train test split!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a similar for loop to before. Except, this time\n",
    "\n",
    "* Use a train test split of 20-80\n",
    "* Instead of looking at $R^2$ and $R^2_{adj}$, look at the MSE for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=3)\n",
    "\n",
    "MSE_test = []\n",
    "MSE_train = []\n",
    "list_n = list(range(5,86,10))\n",
    "for n in list_n: \n",
    "    select_n = RFE(linreg, n_features_to_select = n)\n",
    "    select_n = select_n.fit(X_train, np.ravel(y_train))\n",
    "    selected_columns = X.columns[select_n.support_ ]\n",
    "    linreg.fit(X_train[selected_columns],y_train)\n",
    "    yhat_train = linreg.predict(X_train[selected_columns])\n",
    "    yhat_test = linreg.predict(X_test[selected_columns])\n",
    "    mse_train = np.sum((y_train-yhat_train)**2)/len(y_train)\n",
    "    mse_test =np.sum((y_test-yhat_test)**2)/len(y_test)\n",
    "    print(mse_train)\n",
    "    print(mse_test)\n",
    "MSE_test.append(mse_test)\n",
    "MSE_train.append(mse_train)y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What we see is that both MSE keeps improving when we add variables. It seems like a bigger model improves our performance, and the test and train performance don't really diverge. It is important to note however that is not an unusual result. The performance measures used typically will show this type of behavior. In order to really be able to balance the curse of dimensionality (which will become more important in machine learning), we need other information criteria such as AIC and BIC. You'll learn about them later! Now, let's perform cross-validation on our model with 85 predictors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold cross validation with the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# select 85 best predictors\n",
    "\n",
    "select_85 = RFE(linreg, n_features_to_select = 85)\n",
    "select_85 = select_n.fit(X, np.ravel(y))\n",
    "selected_columns = X.columns[select_n.support_]\n",
    "\n",
    "cv_10_results = cross_val_score(linreg, X[selected_columns], y, cv=10, scoring=\"neg_mean_squared_error\")\n",
    "\n",
    "cv_10_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running our 10-fold cross-validation highlights some issues for sure! Have a look at your list of 10 MSEs. Where most MSEs are manageable, some are very high. The cure of dimensionality is already pretty clear here. The issue is that we have many (dummy) categorical variables that result in columns with many zeroes and few ones. This means that for some folds, there is a risk of ending up with columns that almost exclusively contain 0's for prediction, which might cause weird results. Looking at this, a model with less predictors might make sense again. This is where we conclude for now. It's up to you now to explore other model options! Additionally, it is encouraged to try some of the \"level up\" exercises below. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"how did you pick the question(s) that you did?\"\n",
    "* \"why are these questions important from a business perspective?\"\n",
    "* \"how did you decide on the data cleaning options you performed?\"\n",
    "* \"why did you choose a given method or library?\"\n",
    "* \"why did you select those visualizations and what did you learn from each of them?\"\n",
    "* \"why did you pick those features as predictors?\"\n",
    "* \"how would you interpret the results?\"\n",
    "* \"how confident are you in the predictive quality of the results?\"\n",
    "* \"what are some of the things that could cause the results to be wrong?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project goes through the OSEMN framework..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
